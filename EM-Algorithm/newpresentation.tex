\documentclass[11pt]{beamer}
\usetheme{Warsaw}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Ritwik Sahani/ Aayush Goyal}
\title{The EM Algorithm}
%\setbeamercovered{transparent} 
%\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
%\date{} 
%\subject{} 
\institute{IIT Hyderabad}
\begin{document}

\begin{frame}
\titlepage{}
\end{frame}


\begin{frame}
\textbf{Step-1: Expectation}\linebreak
\linebreak
\textsf{Creates a function for the expectation of the log-likelihood evaluated using the
current estimate for the parameters.}
\linebreak
\linebreak
\linebreak
\linebreak\linebreak
\textbf{Step-2: Maximization}
\linebreak

\textsf{Which computes parameters maximizing the expected log-likelihood found on the
E step. These parameter-estimates are then used to determine the distribution of
the latent variables in the next E step.}

\end{frame}


\begin{frame}
\textbf{Introduction}
\linebreak\linebreak
\textsf{The EM algorithm is used to find (local) maximum likelihood parameters of a
statistical model in cases where the equations cannot be solved directly. Typically
these models involve latent variables in addition to unknown parameters and known
data observations. That is, either missing values exist among the data, or the model
can be formulated more simply by assuming the existence of further unobserved
data points. For example, a mixture model can be described more simply by
assuming that each observed data point has a corresponding unobserved data
point, or latent variable, specifying the mixture component to which each data point
belongs.}
\end{frame}


\begin{frame}
\textbf{Two-Component Mixture Model}\linebreak
\textsf{We consider a simple mixture model for density estimation,
and the corresponding EM algorithm for carrying out maximum likelihood
estimation.}



\begin{figure}
\begin{flushleft}
\includegraphics[scale=0.2]{fig1.jpg}
\label{Histogram of data}
\end{flushleft}



\begin{flushright}
\includegraphics[scale=0.2]{fig2.jpg}
\label{Maximum likelihood fit
of Gaussian densities (solid red) and
responsibility (dotte green) of the left
component density for observation y,
as a function of y}
\end{flushright}
\end{figure}

\end{frame}


\begin{frame}
\begin{figure}
\includegraphics[scale=0.15]{fig3.jpg}
\end{figure}
\textsf{Due to Bimodalty of data Gaussian distribution would be inappropriate. So we
will model Y as a mixture of two distinct distributions.\linebreak\linebreak
$Y_{1} \equiv N(\mu_{1},  \sigma_{12}) $\linebreak
$Y_{2} \equiv N(\mu_{2},  \sigma_{22}) $\linebreak
$Y &= (1-\Delta)*Y_{1} + \Delta * Y_{2} $\linebreak
where $\Delta \in \{0, 1\}$ with $Pr(\Delta &= 1) = \pi $\linebreak\linebreak
This generative representation is explicit: generate a $\Delta \in \{0, 1\} $ with
probability $\pi$, and then depending on the outcome, deliver either Y_{1}  or  Y_{2} .
Let $\phi$ $\theta$ (x) denote the normal density with parameters $\theta = (\mu, \sigma_{2} )$. Then the
density of Y is\linebreak
$gY(y) = (1- \pi)\phi \theta_{1} (y)  + \pi\phi \theta_{2} (y).$}

\end{frame}


\begin{frame}
\textsf{Now suppose we wish to fit this model to the data in Figure 8.5 by maximum likelihood.The parameters are\linebreak
$\theta = (\pi, \theta_{1} , \theta_{2}) = (\pi, \mu_{1} , \sigma_{12} , \mu_{2} , \sigma_{22})$\linebreak
The log-likelihood based on the N training cases is}\linebreak\linebreak
$l(\theta; Z) = \sum_{i=1}^{N}  log[(1- \pi)\phi \theta_{1} (y_{i}) + \pi\phi \theta_{2} (y_{i} )]$\linebreak\linebreak
\textsf{Direct maximization of $l(\theta; Z)$ is quite difficult numerically, because of
the sum of terms inside the logarithm. There is, however, a simpler approach. We consider unobserved latent variables $\theta_{i}$ taking values 0 or 1 as
in (8.36): if $\Delta_{i} = 1 $then $Y_{1}$ comes from model 2, otherwise it comes from
model 1.\linebreak\linebreak
$l_{0}(\theta;Z,\Delta) = \sum_{i=1}^{N}[(1-\Delta_{i})log\phi_{\theta_{1}}(y_{i}) + \Delta_{i}log\phi_{\theta_{2}}(y_{i})] + \sum_{i=1}^{N}[(1-\Delta_{i})log(1-\pi)(y_{i}) + \Delta_{i}log\pi]$}
\end{frame}


\begin{frame}

\textsf{\linebreak\linebreak
Now we apply the E-M Algorithm for Two-component Gaussian mixture}
\begin{figure}
\includegraphics[scale=.315]{fig10.jpg}
\end{figure}
\end{frame}







\end{document}
